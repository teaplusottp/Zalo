{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73bb9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment Ready & Seed Fixed (42)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# Th√™m ƒë∆∞·ªùng d·∫´n code v√†o h·ªá th·ªëng\n",
    "sys.path.append('/code')\n",
    "\n",
    "# Import c√°c th∆∞ vi·ªán model c·ªßa b·∫°n\n",
    "try:\n",
    "    from transformers import AutoImageProcessor, AutoModel\n",
    "    from mobilesamv2.promt_mobilesamv2 import ObjectAwareModel\n",
    "    from mobilesamv2 import SamPredictor\n",
    "    from mobilesamv2.modeling import Sam\n",
    "    from tinyvit.tiny_vit import TinyViT\n",
    "    from mobilesamv2.modeling import PromptEncoder, MaskDecoder, TwoWayTransformer\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è C·∫£nh b√°o Import: {e}. H√£y ch·∫Øc ch·∫Øn b·∫°n ƒë√£ copy folder 'mobilesamv2' v√† 'tinyvit' v√†o /code/\")\n",
    "\n",
    "# H√†m c·ªë ƒë·ªãnh Seed (B·∫Øt bu·ªôc theo quy ƒë·ªãnh BTC)\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"‚úÖ Environment Ready & Seed Fixed (42)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb2bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading Models (This part is NOT timed)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All Models Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "# ====================== CONFIG & CLASSES ======================\n",
    "class DockerConfig:\n",
    "    def __init__(self, video_id):\n",
    "        self.video_id = video_id\n",
    "        # ƒê∆Ø·ªúNG D·∫™N CHU·∫®N TRONG DOCKER\n",
    "        self.data_base = \"D:/code/detect/public_test/public_test/samples\"\n",
    "        self.results_base = \"/result\"\n",
    "        self.segment_base = \"./segment_objects\"\n",
    "\n",
    "        self.video_path = os.path.join(self.data_base, video_id, \"drone_video.mp4\")\n",
    "        self.template_img_dir = os.path.join(self.segment_base, video_id, \"original_images\")\n",
    "        self.template_mask_dir = os.path.join(self.segment_base, video_id, \"mask_images\")\n",
    "\n",
    "        # MODEL PATHS (LOCAL)\n",
    "        self.dino_model_id = \"./weight/DINO\" # Ho·∫∑c folder t√™n model DINOv3\n",
    "        self.sam_checkpoint = './weight/mobile_sam.pt'\n",
    "        self.yolo_model = './weight/ObjectAwareModel.pt'\n",
    "\n",
    "        # THRESHOLDS\n",
    "        self.SCORE_THRESHOLD = 0.50\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.conf_thres = 0.25\n",
    "        self.min_area_ratio = 0.0005\n",
    "        self.max_area_ratio = 0.15\n",
    "        \n",
    "        # TRACKING\n",
    "        self.IOU_THRESHOLD = 0.3\n",
    "        self.MAX_AGE = 10\n",
    "        self.MIN_HITS = 3\n",
    "\n",
    "# --- DINOv3 WRAPPER ---\n",
    "class DinoV3FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_id, device):\n",
    "        super().__init__()\n",
    "        # Load model t·ª´ local path, t·∫Øt check internet\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_id, local_files_only=True)\n",
    "        self.model = AutoModel.from_pretrained(model_id, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        self.patch_size = getattr(self.model.config, \"patch_size\", 14)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, img_pil):\n",
    "        inputs = self.processor(images=img_pil, return_tensors=\"pt\").to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # X·ª≠ l√Ω output DINOv3 ƒë·ªÉ l·∫•y feature map\n",
    "        last_hidden_state = outputs.last_hidden_state \n",
    "        pixel_values = inputs['pixel_values']\n",
    "        h_grid = pixel_values.shape[2] // self.patch_size\n",
    "        w_grid = pixel_values.shape[3] // self.patch_size\n",
    "        num_patches = h_grid * w_grid\n",
    "        \n",
    "        patch_tokens = last_hidden_state[:, 1:, :][:, :num_patches, :]\n",
    "        B, N, C = patch_tokens.shape\n",
    "        \n",
    "        patch_tokens = patch_tokens.permute(0, 2, 1).view(B, C, h_grid, w_grid)\n",
    "        return patch_tokens\n",
    "\n",
    "class FFAProcessor:\n",
    "    @staticmethod\n",
    "    def apply_ffa(feat_map, mask):\n",
    "        target_size = feat_map.shape[-2:]\n",
    "        mask_resized = F.interpolate(mask.float(), size=target_size, mode='nearest')\n",
    "        masked_feat = feat_map * mask_resized\n",
    "        sum_feat = masked_feat.sum(dim=(2, 3))\n",
    "        sum_mask = mask_resized.sum(dim=(2, 3)) + 1e-6\n",
    "        return sum_feat / sum_mask\n",
    "\n",
    "class SimilarityModel:\n",
    "    def __init__(self, cfg):\n",
    "        self.device = cfg.device\n",
    "        self.extractor = DinoV3FeatureExtractor(cfg.dino_model_id, self.device)\n",
    "        self.template_features = None\n",
    "\n",
    "    def load_templates(self, img_dir, mask_dir):\n",
    "        feats = []\n",
    "        for i in range(1, 4):\n",
    "            img_path = os.path.join(img_dir, f\"img_{i}.jpg\")\n",
    "            mask_path = os.path.join(mask_dir, f\"img_{i}.png\")\n",
    "            if not (os.path.exists(img_path) and os.path.exists(mask_path)): continue\n",
    "            \n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            mask = np.array(Image.open(mask_path).convert('L')) > 128\n",
    "            \n",
    "            # Extract feature for template\n",
    "            m = torch.from_numpy(mask.astype(np.float32)).float().unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "            feat_map = self.extractor(img)\n",
    "            feat = FFAProcessor.apply_ffa(feat_map, m)\n",
    "            feats.append(F.normalize(feat, p=2, dim=1).squeeze(0))\n",
    "            \n",
    "        if not feats: return False\n",
    "        self.template_features = torch.stack(feats).to(self.device)\n",
    "        return True\n",
    "\n",
    "    def extract_features(self, img_pil, mask_np):\n",
    "        m = torch.from_numpy(mask_np).float().unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        feat_map = self.extractor(img_pil)\n",
    "        feat = FFAProcessor.apply_ffa(feat_map, m)\n",
    "        return F.normalize(feat, p=2, dim=1).squeeze(0)\n",
    "\n",
    "    def compute_scores(self, feat):\n",
    "        sims = torch.matmul(feat.unsqueeze(0), self.template_features.T).squeeze(0)\n",
    "        avg = sims.mean().item()\n",
    "        best = sims.max().item()\n",
    "        app_bonus = 0.7 * best + 0.3 * avg\n",
    "        return 0, 0, 0, avg, app_bonus\n",
    "\n",
    "    @staticmethod\n",
    "    def size_penalty(bbox, area):\n",
    "        w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        ratio = (w * h) / area\n",
    "        if ratio < 0.0001: s = 0.8\n",
    "        elif ratio <= 0.001: s = 1.0\n",
    "        elif ratio <= 0.01: s = 0.9\n",
    "        elif ratio <= 0.05: s = 0.8\n",
    "        else: s = 0.2\n",
    "        return s\n",
    "\n",
    "# --- SORT TRACKER UTILS ---\n",
    "def iou_batch(bb_test, bb_gt):\n",
    "    if bb_test.size == 0 or bb_gt.size == 0: return np.zeros((bb_test.shape[0], bb_gt.shape[0]))\n",
    "    bb_gt = np.expand_dims(bb_gt, 0)\n",
    "    bb_test = np.expand_dims(bb_test, 1)\n",
    "    xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n",
    "    yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n",
    "    xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n",
    "    yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n",
    "    w = np.maximum(0., xx2 - xx1)\n",
    "    h = np.maximum(0., yy2 - yy1)\n",
    "    wh = w * h\n",
    "    o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1]) + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)\n",
    "    return o\n",
    "\n",
    "class KalmanBoxTracker:\n",
    "    count = 0\n",
    "    def __init__(self, bbox):\n",
    "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "        self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],[0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]], dtype=np.float32)\n",
    "        self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]], dtype=np.float32)\n",
    "        self.kf.P[4:,4:] *= 1000. \n",
    "        self.kf.P *= 10.\n",
    "        self.kf.Q[-1,-1] *= 0.01\n",
    "        self.kf.Q[4:,4:] *= 0.01\n",
    "        self.kf.R[2:,2:] *= 10.\n",
    "        self.kf.x[:4] = self.convert_bbox_to_z(bbox)\n",
    "        self.time_since_update = 0\n",
    "        self.id = KalmanBoxTracker.count\n",
    "        KalmanBoxTracker.count += 1\n",
    "        self.hit_streak = 0\n",
    "        self.last_score = 0.0\n",
    "\n",
    "    def update(self, bbox, score=0.0):\n",
    "        self.time_since_update = 0\n",
    "        self.hit_streak += 1\n",
    "        self.kf.update(self.convert_bbox_to_z(bbox))\n",
    "        self.last_score = score\n",
    "\n",
    "    def predict(self):\n",
    "        if((self.kf.x[6]+self.kf.x[2])<=0): self.kf.x[6] *= 0.0\n",
    "        self.kf.predict()\n",
    "        self.time_since_update += 1\n",
    "        return self.convert_x_to_bbox(self.kf.x)[0]\n",
    "\n",
    "    def get_state(self): return self.convert_x_to_bbox(self.kf.x)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_bbox_to_z(bbox):\n",
    "        w = bbox[2] - bbox[0]; h = bbox[3] - bbox[1]\n",
    "        x = bbox[0] + w/2.; y = bbox[1] + h/2.\n",
    "        return np.array([x, y, w*h, w/float(h)]).reshape((4, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_x_to_bbox(x, score=None):\n",
    "        w = np.sqrt(x[2] * x[3]); h = x[2] / w\n",
    "        return np.array([x[0]-w/2., x[1]-h/2., x[0]+w/2., x[1]+h/2.]).reshape((1,4))\n",
    "\n",
    "def associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n",
    "    if(len(trackers)==0): return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
    "    iou_matrix = iou_batch(detections, trackers)\n",
    "    if min(iou_matrix.shape) > 0:\n",
    "        a = (iou_matrix > iou_threshold).astype(np.int32)\n",
    "        if a.sum(1).max() == 1 and a.sum(0).max() == 1: matched_indices = np.stack(np.where(a), axis=1)\n",
    "        else: matched_indices = np.array(linear_sum_assignment(-iou_matrix)).T\n",
    "    else: matched_indices = np.empty((0,2))\n",
    "    unmatched_detections = [d for d in range(len(detections)) if d not in matched_indices[:,0]]\n",
    "    unmatched_trackers = [t for t in range(len(trackers)) if t not in matched_indices[:,1]]\n",
    "    matches = []\n",
    "    for m in matched_indices:\n",
    "        if(iou_matrix[m[0], m[1]] < iou_threshold):\n",
    "            unmatched_detections.append(m[0]); unmatched_trackers.append(m[1])\n",
    "        else: matches.append(m.reshape(1,2))\n",
    "    if(len(matches)==0): matches = np.empty((0,2),dtype=int)\n",
    "    else: matches = np.concatenate(matches,axis=0)\n",
    "    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
    "\n",
    "# ====================== LOAD MODELS ======================\n",
    "print(\"‚è≥ Loading Models (This part is NOT timed)...\")\n",
    "dummy_cfg = DockerConfig(\"init\")\n",
    "\n",
    "# 1. DINOv3\n",
    "sim_model = SimilarityModel(dummy_cfg)\n",
    "\n",
    "# 2. YOLO\n",
    "yolo = ObjectAwareModel(dummy_cfg.yolo_model)\n",
    "\n",
    "# 3. SAM\n",
    "sam = Sam(image_encoder=TinyViT(img_size=1024, in_chans=3, num_classes=1000,\n",
    "                                embed_dims=[64,128,160,320], depths=[2,2,6,2],\n",
    "                                num_heads=[2,4,5,10], window_sizes=[7,7,14,7]),\n",
    "          prompt_encoder=PromptEncoder(embed_dim=256, image_embedding_size=(64,64),\n",
    "                                       input_image_size=(1024,1024), mask_in_chans=16),\n",
    "          mask_decoder=MaskDecoder(num_multimask_outputs=3,\n",
    "                                   transformer=TwoWayTransformer(depth=2, embedding_dim=256, mlp_dim=2048, num_heads=8),\n",
    "                                   transformer_dim=256))\n",
    "sam.load_state_dict(torch.load(dummy_cfg.sam_checkpoint, map_location=dummy_cfg.device), strict=False)\n",
    "sam.to(dummy_cfg.device).eval()\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "print(\"‚úÖ All Models Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688613a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Found 6 videos: ['BlackBox_0', 'BlackBox_1', 'CardboardBox_0', 'CardboardBox_1', 'LifeJacket_0', 'LifeJacket_1']\n",
      "‚ñ∂Ô∏è Processing: BlackBox_0\n"
     ]
    }
   ],
   "source": [
    "# T√¨m danh s√°ch video input\n",
    "search_path = \"D:/code/detect/public_test/public_test/samples/*\"\n",
    "test_cases = [os.path.basename(p) for p in glob.glob(search_path) if os.path.isdir(p)]\n",
    "print(f\"üîé Found {len(test_cases)} videos: {test_cases}\")\n",
    "\n",
    "all_predicted_time = []\n",
    "all_results_json = {} \n",
    "\n",
    "for video_id in test_cases:\n",
    "    print(f\"‚ñ∂Ô∏è Processing: {video_id}\")\n",
    "    \n",
    "    # --- B·∫ÆT ƒê·∫¶U ƒêO TH·ªúI GIAN (T1) ---\n",
    "    t1 = time()\n",
    "    \n",
    "    # 1. Config cho Video n√†y\n",
    "    cfg = DockerConfig(video_id)\n",
    "    \n",
    "    # 2. Load Template cho Video n√†y\n",
    "    templates_loaded = sim_model.load_templates(cfg.template_img_dir, cfg.template_mask_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(cfg.video_path)\n",
    "    W, H = int(cap.get(3)), int(cap.get(4))\n",
    "    \n",
    "    # Reset Tracker\n",
    "    trackers = []\n",
    "    KalmanBoxTracker.count = 0\n",
    "    video_predictions = []\n",
    "    frame_idx = 0\n",
    "    \n",
    "    # --- V√íNG L·∫∂P X·ª¨ L√ù FRAME (FULL VIDEO) ---\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break # Ch·∫°y ƒë·∫øn khi h·∫øt video th√¨ d·ª´ng\n",
    "        \n",
    "        best_obj = None\n",
    "        \n",
    "        # Ch·ªâ x·ª≠ l√Ω n·∫øu ƒë√£ load ƒë∆∞·ª£c template\n",
    "        if templates_loaded:\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # A. Detect Candidates (YOLO)\n",
    "            results = yolo(rgb, conf=cfg.conf_thres, verbose=False)\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy() if results and results[0].boxes is not None else []\n",
    "            \n",
    "            high_score_candidates = []\n",
    "            if len(boxes) > 0:\n",
    "                predictor.set_image(rgb)\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    # L·ªçc Area Ratio\n",
    "                    if not (cfg.min_area_ratio <= ((x2-x1)*(y2-y1))/(W*H) <= cfg.max_area_ratio): continue\n",
    "                    \n",
    "                    # SAM Segment\n",
    "                    masks, _, _ = predictor.predict(box=box, multimask_output=False)\n",
    "                    if len(masks) == 0: continue\n",
    "                    mask = masks[0]\n",
    "                    \n",
    "                    # Crop & Feature\n",
    "                    yy, xx = np.where(mask)\n",
    "                    if len(yy) == 0: continue\n",
    "                    y1b, y2b, x1b, x2b = yy.min(), yy.max()+1, xx.min(), xx.max()+1\n",
    "                    crop = rgb[y1b:y2b, x1b:x2b]\n",
    "                    crop_mask = mask[y1b:y2b, x1b:x2b]\n",
    "                    \n",
    "                    # Scoring\n",
    "                    feat = sim_model.extract_features(Image.fromarray(crop), crop_mask)\n",
    "                    _, _, _, avg_match, app_bonus = sim_model.compute_scores(feat)\n",
    "                    size_pen = sim_model.size_penalty([x1b, y1b, x2b, y2b], W * H)\n",
    "                    \n",
    "                    final_score = 0.7 * avg_match + 0.25 * app_bonus + 0.05 * size_pen\n",
    "                    \n",
    "                    if final_score > cfg.SCORE_THRESHOLD:\n",
    "                        high_score_candidates.append({'bbox': np.array([x1b, y1b, x2b, y2b]), 'score': final_score})\n",
    "            \n",
    "            # B. Update Trackers (SORT)\n",
    "            if len(high_score_candidates) > 0: dets = np.array([c['bbox'] for c in high_score_candidates])\n",
    "            else: dets = np.empty((0, 4))\n",
    "            \n",
    "            trks = np.zeros((len(trackers), 4))\n",
    "            to_del = []\n",
    "            for t, trk in enumerate(trackers):\n",
    "                pos = trk.predict()\n",
    "                trks[t, :] = [pos[0], pos[1], pos[2], pos[3]]\n",
    "                if np.any(np.isnan(pos)): to_del.append(t)\n",
    "            for t in reversed(to_del): trackers.pop(t)\n",
    "\n",
    "            matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets, trks, iou_threshold=cfg.IOU_THRESHOLD)\n",
    "\n",
    "            for m in matched:\n",
    "                trackers[m[1]].update(dets[m[0]], score=high_score_candidates[m[0]]['score'])\n",
    "            for i in unmatched_dets:\n",
    "                trk = KalmanBoxTracker(dets[i])\n",
    "                trk.update(dets[i], score=high_score_candidates[i]['score'])\n",
    "                trackers.append(trk)\n",
    "                \n",
    "            # C. Select Best Object\n",
    "            active_trackers = []\n",
    "            for trk in trackers:\n",
    "                 # Logic: Ch·ªâ l·∫•y object b√°m t·ªët ho·∫∑c frame ƒë·∫ßu video\n",
    "                 if (trk.time_since_update < 1) and (trk.hit_streak >= cfg.MIN_HITS or frame_idx <= 5): \n",
    "                    active_trackers.append({\"bbox\": trk.get_state()[0], \"score\": trk.last_score, \"id\": trk.id})\n",
    "            \n",
    "            if active_trackers:\n",
    "                best_obj = max(active_trackers, key=lambda x: x['score'])\n",
    "\n",
    "        # Save Frame Result\n",
    "        video_predictions.append({\n",
    "            \"frame_id\": frame_idx,\n",
    "            \"box\": best_obj['bbox'].astype(int).tolist() if best_obj else [],\n",
    "            \"score\": float(best_obj['score']) if best_obj else 0.0,\n",
    "            \"track_id\": int(best_obj['id']) if best_obj else -1\n",
    "        })\n",
    "        frame_idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    # --- K·∫æT TH√öC ƒêO TH·ªúI GIAN (T2) ---\n",
    "    t2 = time()\n",
    "    predicted_time = int((t2 - t1) * 1000)\n",
    "    \n",
    "    # L∆∞u k·∫øt qu·∫£\n",
    "    all_predicted_time.append({\"id\": video_id, \"answer\": \"processed\", \"time\": predicted_time})\n",
    "    all_results_json[video_id] = video_predictions\n",
    "\n",
    "# --- GHI FILE OUTPUT ---\n",
    "output_dir = \"/result\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. File th·ªùi gian\n",
    "df_time = pd.DataFrame(all_predicted_time)\n",
    "df_time.to_csv(os.path.join(output_dir, \"time_submission.csv\"), index=False)\n",
    "\n",
    "# 2. File k·∫øt qu·∫£ d·ª± ƒëo√°n\n",
    "with open(os.path.join(output_dir, \"jupyter_submission.json\"), 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ DONE! Results saved to /result/\")\n",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e00958",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
