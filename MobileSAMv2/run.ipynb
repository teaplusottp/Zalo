{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73bb9ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ c·ªë ƒë·ªãnh Seed: 42\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from filterpy.kalman import KalmanFilter\n",
    "\n",
    "# --- IMPORT C√ÅC TH∆Ø VI·ªÜN C·ª¶A B·∫†N (ƒê·∫£m b·∫£o folder n·∫±m trong /code) ---\n",
    "sys.path.append('/code')\n",
    "# L∆∞u √Ω: C√°c d√≤ng import d∆∞·ªõi ƒë√¢y ph·ª• thu·ªôc v√†o folder code b·∫°n copy v√†o docker\n",
    "try:\n",
    "    from transformers import AutoImageProcessor, AutoModel\n",
    "    from mobilesamv2.promt_mobilesamv2 import ObjectAwareModel\n",
    "    from mobilesamv2 import SamPredictor\n",
    "    from mobilesamv2.modeling import Sam\n",
    "    from tinyvit.tiny_vit import TinyViT\n",
    "    from mobilesamv2.modeling import PromptEncoder, MaskDecoder, TwoWayTransformer\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå L·ªói Import: {e}. H√£y ki·ªÉm tra l·∫°i c·∫•u tr√∫c folder trong Docker.\")\n",
    "\n",
    "# --- H√ÄM SEED ---\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "print(\"‚úÖ ƒê√£ c·ªë ƒë·ªãnh Seed: 42\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb2bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang load models...\n",
      "‚úÖ Load xong to√†n b·ªô Model!\n"
     ]
    }
   ],
   "source": [
    "# ====================== C·∫§U H√åNH DOCKER ======================\n",
    "class DockerConfig:\n",
    "    def __init__(self, video_id=\"test\"):\n",
    "        # C·∫§U H√åNH PATH CHU·∫®N C·ª¶A BTC\n",
    "        self.video_id = video_id\n",
    "        self.data_base = \"D:/code/detect/public_test/public_test/samples\"           # Input folder\n",
    "        self.results_base = \"/result\"             # Output folder\n",
    "        self.segment_base = \"/segment_objects\" # Template folder (copy v√†o docker)\n",
    "\n",
    "        self.video_path = os.path.join(self.data_base, video_id, \"drone_video.mp4\")\n",
    "        \n",
    "        # Template images (d√πng ƒë·ªÉ so s√°nh features)\n",
    "        self.template_img_dir = os.path.join(self.segment_base, video_id, \"original_images\")\n",
    "        self.template_mask_dir = os.path.join(self.segment_base, video_id, \"mask_images\")\n",
    "\n",
    "        # MODEL PATHS (LOCAL - KH√îNG D√ôNG HUGGINGFACE HUB)\n",
    "        self.dino_model_id = \"./weight/DINO\" \n",
    "        self.sam_checkpoint = './weight/mobile_sam.pt'\n",
    "        self.yolo_model = './weight/ObjectAwareModel.pt'\n",
    "\n",
    "        # THRESHOLDS\n",
    "        self.SCORE_THRESHOLD = 0.50\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.conf_thres = 0.25\n",
    "        self.min_area_ratio = 0.0005\n",
    "        self.max_area_ratio = 0.15\n",
    "        \n",
    "        # TRACKING CONFIG\n",
    "        self.IOU_THRESHOLD = 0.3\n",
    "        self.MAX_AGE = 10\n",
    "        self.MIN_HITS = 3\n",
    "\n",
    "# ====================== HELPER CLASSES (COPY T·ª™ CODE B·∫†N) ======================\n",
    "\n",
    "class DinoV3FeatureExtractor(nn.Module):\n",
    "    def __init__(self, model_id, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Load t·ª´ ƒë∆∞·ªùng d·∫´n local\n",
    "        self.processor = AutoImageProcessor.from_pretrained(model_id, local_files_only=True)\n",
    "        self.model = AutoModel.from_pretrained(model_id, local_files_only=True).to(device)\n",
    "        self.model.eval()\n",
    "        self.patch_size = getattr(self.model.config, \"patch_size\", 14)\n",
    "        for param in self.model.parameters(): param.requires_grad = False\n",
    "\n",
    "    def forward(self, img_pil):\n",
    "        inputs = self.processor(images=img_pil, return_tensors=\"pt\").to(self.device)\n",
    "        pixel_values = inputs['pixel_values']\n",
    "        h_img, w_img = pixel_values.shape[2], pixel_values.shape[3]\n",
    "        h_grid = h_img // self.patch_size\n",
    "        w_grid = w_img // self.patch_size\n",
    "        num_patches = h_grid * w_grid\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        last_hidden_state = outputs.last_hidden_state \n",
    "        tokens_no_cls = last_hidden_state[:, 1:, :]\n",
    "        patch_tokens = tokens_no_cls[:, :num_patches, :]\n",
    "        \n",
    "        B, N, C = patch_tokens.shape\n",
    "        if N != num_patches:\n",
    "            patch_tokens = patch_tokens.permute(0, 2, 1).view(B, C, int(np.sqrt(N)), int(np.sqrt(N)))\n",
    "            patch_tokens = F.interpolate(patch_tokens, size=(h_grid, w_grid), mode='bilinear')\n",
    "            return patch_tokens\n",
    "\n",
    "        feat_map = patch_tokens.reshape(B, h_grid, w_grid, C).permute(0, 3, 1, 2)\n",
    "        return feat_map\n",
    "\n",
    "class FFAProcessor:\n",
    "    @staticmethod\n",
    "    def apply_ffa(feat_map, mask):\n",
    "        target_size = feat_map.shape[-2:]\n",
    "        mask_resized = F.interpolate(mask.float(), size=target_size, mode='nearest')\n",
    "        masked_feat = feat_map * mask_resized\n",
    "        sum_feat = masked_feat.sum(dim=(2, 3))\n",
    "        sum_mask = mask_resized.sum(dim=(2, 3)) + 1e-6\n",
    "        return sum_feat / sum_mask\n",
    "\n",
    "class SimilarityModel:\n",
    "    def __init__(self, cfg):\n",
    "        self.device = cfg.device\n",
    "        self.extractor = DinoV3FeatureExtractor(cfg.dino_model_id, self.device)\n",
    "        self.template_features = None\n",
    "\n",
    "    def load_templates(self, img_dir, mask_dir):\n",
    "        feats = []\n",
    "        # Load 3 templates img_1, img_2, img_3\n",
    "        for i in range(1, 4):\n",
    "            img_path = os.path.join(img_dir, f\"img_{i}.jpg\")\n",
    "            mask_path = os.path.join(mask_dir, f\"img_{i}.png\")\n",
    "            if not (os.path.exists(img_path) and os.path.exists(mask_path)): continue\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            mask = np.array(Image.open(mask_path).convert('L')) > 128\n",
    "            feat = self.extract_features(img, mask.astype(np.float32))\n",
    "            feats.append(feat)\n",
    "            \n",
    "        if not feats: return False # Tr·∫£ v·ªÅ False n·∫øu kh√¥ng load ƒë∆∞·ª£c template n√†o\n",
    "        self.template_features = torch.stack(feats).to(self.device)\n",
    "        self.template_features = F.normalize(self.template_features, p=2, dim=1)\n",
    "        return True\n",
    "\n",
    "    def extract_features(self, img_pil, mask_np):\n",
    "        m = torch.from_numpy(mask_np).float().unsqueeze(0).unsqueeze(0).to(self.device)\n",
    "        feat_map = self.extractor(img_pil)\n",
    "        feat = FFAProcessor.apply_ffa(feat_map, m)\n",
    "        return F.normalize(feat, p=2, dim=1).squeeze(0)\n",
    "\n",
    "    def compute_scores(self, feat):\n",
    "        sims = torch.matmul(feat.unsqueeze(0), self.template_features.T).squeeze(0)\n",
    "        s1, s2, s3 = sims.cpu().numpy() if sims.shape[0] >= 3 else (0,0,0) # Handle tr∆∞·ªùng h·ª£p thi·∫øu template\n",
    "        avg = sims.mean().item()\n",
    "        best = sims.max().item()\n",
    "        app_bonus = 0.7 * best + 0.3 * avg\n",
    "        return s1, s2, s3, avg, app_bonus\n",
    "    \n",
    "    @staticmethod\n",
    "    def size_penalty(bbox, area):\n",
    "        w, h = bbox[2] - bbox[0], bbox[3] - bbox[1]\n",
    "        ratio = (w * h) / area\n",
    "        if ratio < 0.0001: s = 0.8\n",
    "        elif ratio <= 0.001: s = 1.0\n",
    "        elif ratio <= 0.01: s = 0.9\n",
    "        elif ratio <= 0.05: s = 0.8\n",
    "        else: s = 0.2\n",
    "        return s\n",
    "\n",
    "# --- SORT TRACKING UTILS ---\n",
    "def iou_batch(bb_test, bb_gt):\n",
    "    if bb_test.size == 0 or bb_gt.size == 0:\n",
    "        return np.zeros((bb_test.shape[0], bb_gt.shape[0]))\n",
    "    bb_gt = np.expand_dims(bb_gt, 0)\n",
    "    bb_test = np.expand_dims(bb_test, 1)\n",
    "    xx1 = np.maximum(bb_test[..., 0], bb_gt[..., 0])\n",
    "    yy1 = np.maximum(bb_test[..., 1], bb_gt[..., 1])\n",
    "    xx2 = np.minimum(bb_test[..., 2], bb_gt[..., 2])\n",
    "    yy2 = np.minimum(bb_test[..., 3], bb_gt[..., 3])\n",
    "    w = np.maximum(0., xx2 - xx1)\n",
    "    h = np.maximum(0., yy2 - yy1)\n",
    "    wh = w * h\n",
    "    o = wh / ((bb_test[..., 2] - bb_test[..., 0]) * (bb_test[..., 3] - bb_test[..., 1]) \n",
    "        + (bb_gt[..., 2] - bb_gt[..., 0]) * (bb_gt[..., 3] - bb_gt[..., 1]) - wh)\n",
    "    return o\n",
    "\n",
    "class KalmanBoxTracker:\n",
    "    count = 0\n",
    "    def __init__(self, bbox):\n",
    "        self.kf = KalmanFilter(dim_x=7, dim_z=4)\n",
    "        self.kf.F = np.array([[1,0,0,0,1,0,0],[0,1,0,0,0,1,0],[0,0,1,0,0,0,1],[0,0,0,1,0,0,0],  \n",
    "                              [0,0,0,0,1,0,0],[0,0,0,0,0,1,0],[0,0,0,0,0,0,1]], dtype=np.float32)\n",
    "        self.kf.H = np.array([[1,0,0,0,0,0,0],[0,1,0,0,0,0,0],[0,0,1,0,0,0,0],[0,0,0,1,0,0,0]], dtype=np.float32)\n",
    "        self.kf.P[4:,4:] *= 1000. \n",
    "        self.kf.P *= 10.\n",
    "        self.kf.Q[-1,-1] *= 0.01\n",
    "        self.kf.Q[4:,4:] *= 0.01\n",
    "        self.kf.R[2:,2:] *= 10.\n",
    "        self.kf.x[:4] = self.convert_bbox_to_z(bbox)\n",
    "        self.time_since_update = 0\n",
    "        self.id = KalmanBoxTracker.count\n",
    "        KalmanBoxTracker.count += 1\n",
    "        self.history = []\n",
    "        self.hits = 0\n",
    "        self.hit_streak = 0\n",
    "        self.age = 0\n",
    "        self.last_score = 0.0\n",
    "\n",
    "    def update(self, bbox, score=0.0):\n",
    "        self.time_since_update = 0\n",
    "        self.history = []\n",
    "        self.hits += 1\n",
    "        self.hit_streak += 1\n",
    "        self.kf.update(self.convert_bbox_to_z(bbox))\n",
    "        self.last_score = score\n",
    "\n",
    "    def predict(self):\n",
    "        if((self.kf.x[6]+self.kf.x[2])<=0):\n",
    "            self.kf.x[6] *= 0.0\n",
    "        self.kf.predict()\n",
    "        self.age += 1\n",
    "        if(self.time_since_update>0):\n",
    "            self.hit_streak = 0\n",
    "        self.time_since_update += 1\n",
    "        self.history.append(self.convert_x_to_bbox(self.kf.x))\n",
    "        return self.history[-1]\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.convert_x_to_bbox(self.kf.x)\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_bbox_to_z(bbox):\n",
    "        w = bbox[2] - bbox[0]\n",
    "        h = bbox[3] - bbox[1]\n",
    "        x = bbox[0] + w/2.\n",
    "        y = bbox[1] + h/2.\n",
    "        s = w * h\n",
    "        r = w / float(h)\n",
    "        return np.array([x, y, s, r]).reshape((4, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def convert_x_to_bbox(x, score=None):\n",
    "        w = np.sqrt(x[2] * x[3])\n",
    "        h = x[2] / w\n",
    "        return np.array([x[0]-w/2., x[1]-h/2., x[0]+w/2., x[1]+h/2.]).reshape((1,4))\n",
    "\n",
    "def associate_detections_to_trackers(detections, trackers, iou_threshold=0.3):\n",
    "    if(len(trackers)==0):\n",
    "        return np.empty((0,2),dtype=int), np.arange(len(detections)), np.empty((0,5),dtype=int)\n",
    "    iou_matrix = iou_batch(detections, trackers)\n",
    "    if min(iou_matrix.shape) > 0:\n",
    "        a = (iou_matrix > iou_threshold).astype(np.int32)\n",
    "        if a.sum(1).max() == 1 and a.sum(0).max() == 1:\n",
    "            matched_indices = np.stack(np.where(a), axis=1)\n",
    "        else:\n",
    "            matched_indices = linear_sum_assignment(-iou_matrix)\n",
    "            matched_indices = np.array(matched_indices).T\n",
    "    else:\n",
    "        matched_indices = np.empty((0,2))\n",
    "    unmatched_detections = []\n",
    "    for d, det in enumerate(detections):\n",
    "        if(d not in matched_indices[:,0]): unmatched_detections.append(d)\n",
    "    unmatched_trackers = []\n",
    "    for t, trk in enumerate(trackers):\n",
    "        if(t not in matched_indices[:,1]): unmatched_trackers.append(t)\n",
    "    matches = []\n",
    "    for m in matched_indices:\n",
    "        if(iou_matrix[m[0], m[1]] < iou_threshold):\n",
    "            unmatched_detections.append(m[0])\n",
    "            unmatched_trackers.append(m[1])\n",
    "        else:\n",
    "            matches.append(m.reshape(1,2))\n",
    "    if(len(matches)==0): matches = np.empty((0,2),dtype=int)\n",
    "    else: matches = np.concatenate(matches,axis=0)\n",
    "    return matches, np.array(unmatched_detections), np.array(unmatched_trackers)\n",
    "\n",
    "# ====================== LOADING MODELS ======================\n",
    "dummy_cfg = DockerConfig(\"Lifejacket_0\")\n",
    "print(\"‚è≥ ƒêang load models...\")\n",
    "\n",
    "# 1. Sim Model\n",
    "sim_model = SimilarityModel(dummy_cfg)\n",
    "\n",
    "# 2. YOLO\n",
    "yolo = ObjectAwareModel(dummy_cfg.yolo_model)\n",
    "\n",
    "# 3. SAM\n",
    "sam = Sam(image_encoder=TinyViT(img_size=1024, in_chans=3, num_classes=1000,\n",
    "                                embed_dims=[64,128,160,320], depths=[2,2,6,2],\n",
    "                                num_heads=[2,4,5,10], window_sizes=[7,7,14,7]),\n",
    "          prompt_encoder=PromptEncoder(embed_dim=256, image_embedding_size=(64,64),\n",
    "                                       input_image_size=(1024,1024), mask_in_chans=16),\n",
    "          mask_decoder=MaskDecoder(num_multimask_outputs=3,\n",
    "                                   transformer=TwoWayTransformer(depth=2, embedding_dim=256, mlp_dim=2048, num_heads=8),\n",
    "                                   transformer_dim=256))\n",
    "sam.load_state_dict(torch.load(dummy_cfg.sam_checkpoint, map_location=dummy_cfg.device), strict=False)\n",
    "sam.to(dummy_cfg.device).eval()\n",
    "predictor = SamPredictor(sam)\n",
    "\n",
    "print(\"‚úÖ Load xong to√†n b·ªô Model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688613a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé T√¨m th·∫•y 6 videos: ['BlackBox_0', 'BlackBox_1', 'CardboardBox_0', 'CardboardBox_1', 'LifeJacket_0', 'LifeJacket_1']\n",
      "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: BlackBox_0\n",
      "‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: BlackBox_1\n"
     ]
    }
   ],
   "source": [
    "# T√¨m t·∫•t c·∫£ folder test cases trong /data/samples\n",
    "search_path = \"D:/code/detect/public_test/public_test/samples/*\"\n",
    "test_cases = [os.path.basename(p) for p in glob.glob(search_path) if os.path.isdir(p)]\n",
    "print(f\"üîé T√¨m th·∫•y {len(test_cases)} videos: {test_cases}\")\n",
    "\n",
    "all_predicted_time = []\n",
    "all_results_json = {} # L∆∞u k·∫øt qu·∫£ format c·ªßa BTC\n",
    "\n",
    "for video_id in test_cases:\n",
    "    print(f\"‚ñ∂Ô∏è ƒêang x·ª≠ l√Ω: {video_id}\")\n",
    "    \n",
    "    # --- B·∫ÆT ƒê·∫¶U T√çNH GI·ªú ---\n",
    "    t1 = time()\n",
    "    \n",
    "    # Setup Config cho video hi·ªán t·∫°i\n",
    "    cfg = DockerConfig(video_id)\n",
    "    \n",
    "    # Load Templates (N·∫øu th·∫•t b·∫°i th√¨ skip ho·∫∑c tr·∫£ v·ªÅ r·ªóng)\n",
    "    templates_loaded = sim_model.load_templates(cfg.template_img_dir, cfg.template_mask_dir)\n",
    "    \n",
    "    cap = cv2.VideoCapture(cfg.video_path)\n",
    "    W, H = int(cap.get(3)), int(cap.get(4))\n",
    "    \n",
    "    # Reset Tracker cho video m·ªõi\n",
    "    trackers = []\n",
    "    KalmanBoxTracker.count = 0\n",
    "    video_predictions = [] # K·∫øt qu·∫£ c·ªßa video n√†y\n",
    "    frame_idx = 0\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret: break\n",
    "        \n",
    "        # Ch·ªâ x·ª≠ l√Ω n·∫øu load template th√†nh c√¥ng, n·∫øu kh√¥ng th√¨ tr·∫£ v·ªÅ empty\n",
    "        best_obj = None\n",
    "        if templates_loaded:\n",
    "            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # 1. YOLO Detect\n",
    "            results = yolo(rgb, conf=cfg.conf_thres, verbose=False)\n",
    "            boxes = results[0].boxes.xyxy.cpu().numpy() if results and results[0].boxes is not None else []\n",
    "            \n",
    "            high_score_candidates = []\n",
    "            if len(boxes) > 0:\n",
    "                predictor.set_image(rgb)\n",
    "                for box in boxes:\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    if not (cfg.min_area_ratio <= (x2-x1)*(y2-y1)/(W*H) <= cfg.max_area_ratio): continue\n",
    "\n",
    "                    masks, _, _ = predictor.predict(box=box, multimask_output=False)\n",
    "                    if len(masks) == 0: continue\n",
    "                    mask = masks[0]\n",
    "                    \n",
    "                    yy, xx = np.where(mask)\n",
    "                    if len(yy) == 0: continue\n",
    "                    y1b, y2b, x1b, x2b = yy.min(), yy.max()+1, xx.min(), xx.max()+1\n",
    "                    crop = rgb[y1b:y2b, x1b:x2b]\n",
    "                    crop_mask = mask[y1b:y2b, x1b:x2b]\n",
    "                    \n",
    "                    feat = sim_model.extract_features(Image.fromarray(crop), crop_mask)\n",
    "                    s1, s2, s3, avg_match, app_bonus = sim_model.compute_scores(feat)\n",
    "                    size_pen = sim_model.size_penalty([x1b, y1b, x2b, y2b], W * H)\n",
    "                    final_score = 0.7 * avg_match + 0.25 * app_bonus + 0.05 * size_pen\n",
    "                    \n",
    "                    if final_score > cfg.SCORE_THRESHOLD:\n",
    "                        high_score_candidates.append({\n",
    "                            'bbox': np.array([x1b, y1b, x2b, y2b]), \n",
    "                            'score': final_score\n",
    "                        })\n",
    "            \n",
    "            # 2. SORT Logic\n",
    "            if len(high_score_candidates) > 0:\n",
    "                dets_for_track = np.array([c['bbox'] for c in high_score_candidates])\n",
    "            else:\n",
    "                dets_for_track = np.empty((0, 4))\n",
    "                \n",
    "            trks_for_track = np.zeros((len(trackers), 4))\n",
    "            to_del = []\n",
    "            for t, trk in enumerate(trackers):\n",
    "                pos = trk.predict()[0]\n",
    "                trks_for_track[t, :] = [pos[0], pos[1], pos[2], pos[3]]\n",
    "                if np.any(np.isnan(pos)): to_del.append(t)\n",
    "            for t in reversed(to_del): trackers.pop(t)\n",
    "\n",
    "            matched, unmatched_dets, unmatched_trks = associate_detections_to_trackers(dets_for_track, trks_for_track, iou_threshold=cfg.IOU_THRESHOLD)\n",
    "\n",
    "            for m in matched:\n",
    "                trackers[m[1]].update(dets_for_track[m[0]], score=high_score_candidates[m[0]]['score'])\n",
    "            for i in unmatched_dets:\n",
    "                trk = KalmanBoxTracker(dets_for_track[i])\n",
    "                trk.update(dets_for_track[i], score=high_score_candidates[i]['score'])\n",
    "                trackers.append(trk)\n",
    "\n",
    "            # Get Best Tracker\n",
    "            active_trackers = []\n",
    "            for trk in trackers:\n",
    "                 # Logic visualize/save: hit_streak >= MIN_HITS or frame ƒë·∫ßu\n",
    "                 if (trk.time_since_update < 1) and (trk.hit_streak >= cfg.MIN_HITS or frame_idx <= 5): \n",
    "                    active_trackers.append({\"bbox\": trk.get_state()[0], \"score\": trk.last_score, \"id\": trk.id})\n",
    "            \n",
    "            if active_trackers:\n",
    "                best_obj = max(active_trackers, key=lambda x: x['score'])\n",
    "        \n",
    "        # Save Result\n",
    "        frame_res = {\n",
    "            \"frame_id\": frame_idx,\n",
    "            \"box\": best_obj['bbox'].astype(int).tolist() if best_obj else [],\n",
    "            \"score\": float(best_obj['score']) if best_obj else 0.0,\n",
    "            \"track_id\": int(best_obj['id']) if best_obj else -1\n",
    "        }\n",
    "        video_predictions.append(frame_res)\n",
    "        frame_idx += 1\n",
    "        \n",
    "    cap.release()\n",
    "    # --- K·∫æT TH√öC T√çNH GI·ªú ---\n",
    "    t2 = time()\n",
    "    \n",
    "    predicted_time = int((t2 - t1) * 1000) # milliseconds\n",
    "    \n",
    "    # L∆∞u time\n",
    "    all_predicted_time.append({\"id\": video_id, \"answer\": \"processed\", \"time\": predicted_time})\n",
    "    \n",
    "    # L∆∞u json prediction (Tu·ª≥ format BTC y√™u c·∫ßu, ·ªü ƒë√¢y l√† list c√°c object)\n",
    "    all_results_json[video_id] = video_predictions\n",
    "\n",
    "# --- GHI FILE OUTPUT ---\n",
    "output_dir = \"/result\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 1. Ghi file time_submission.csv [cite: 513]\n",
    "df_time = pd.DataFrame(all_predicted_time)\n",
    "# C·ªôt b·∫Øt bu·ªôc: id, answer, time\n",
    "df_time.to_csv(os.path.join(output_dir, \"time_submission.csv\"), index=False)\n",
    "\n",
    "# 2. Ghi file jupyter_submission.json [cite: 514]\n",
    "# Format json cu·ªëi c√πng: {\"video_id\": ..., \"predictions\": ...} ho·∫∑c List, tu·ª≥ ƒë·ªÅ b√†i.\n",
    "# D·ª±a tr√™n code c≈© c·ªßa b·∫°n th√¨ m·ªói video ra 1 file json, nh∆∞ng file n·ªôp t·ªïng h·ª£p th∆∞·ªùng g·ªôp l·∫°i.\n",
    "# ·ªû ƒë√¢y m√¨nh dump dict t·ªïng h·ª£p.\n",
    "with open(os.path.join(output_dir, \"jupyter_submission.json\"), 'w') as f:\n",
    "    json.dump(all_results_json, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ HO√ÄN TH√ÄNH! K·∫øt qu·∫£ ƒë√£ l∆∞u t·∫°i /result/\")\n",
    "print(df_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291e1303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
